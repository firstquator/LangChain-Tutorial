{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📖 Stuff Documents 체인을 사용하여 완전한 RAG 파이프라인을 구현\n",
    "\n",
    "- 체인을 `수동으로 구현`해야 합니다.\n",
    "\n",
    "- 체인에 `ConversationBufferMemory` 를 부여합니다.\n",
    "\n",
    "- 해당 문서를 사용하여 RAG를 수행합니다 : https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\n",
    "\n",
    "- 체인에 다음 질문을 합니다:\n",
    "    - Aaronson 은 유죄인가요?\n",
    "    - 그가 테이블에 어떤 메시지를 썼나요?\n",
    "    - Julia 는 누구인가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 사용할 라이브러리 및 모듈 import 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 언어모델(LLM) 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,        # 창의성 (0 ~ 2)\n",
    "    model='gpt-3.5-turbo',  # 사용 모델 지정 (Default : gpt-3.5-turbo)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 메모리 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,           # 문자열 기반이 아닌, ChatPromptTemplate 에서 사용할 수 있는 형태로 반환\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 1단계 : `문서 로드 (Load Documents)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"./files/challenge.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 2단계 : `텍스트 분할 (Split Text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",                 # 특정 기준으로 분할\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "split_doc = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 3단계 : `임베딩 (Embedding) 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 캐시 지정\n",
    "cache_dir = LocalFileStore('./.cache/')\n",
    "\n",
    "# 캐시를 활용한 임베딩\n",
    "cacehd_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 4단계 : `DB 생성 및 저장`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=split_doc, embedding=cacehd_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 5단계 : `검색기(Retriever) 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 6단계 : `프롬프트(prompt) 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved {context} to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Answer in Korean.\n",
    "    \"\"\"\n",
    "    ),\n",
    "    # MessagesPlaceholder(variable_name='history'),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 7단계 : `Stuff Documenet Chain 안에 Memory를 연결한 Class 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StuffDocumentMemoryChain:\n",
    "    def __init__(self, llm, prompt, memory, retriver, input_key=\"question\"):\n",
    "        self.prompt = prompt\n",
    "        self.memory = memory\n",
    "        self.retriver = retriver\n",
    "        self.input_key = input_key\n",
    "        self.chain = {\n",
    "            'context': retriver, \n",
    "            # 'history': RunnablePassthrough.assign(history=self.load_memory),\n",
    "            'question': RunnablePassthrough(),\n",
    "        } | prompt | llm\n",
    "\n",
    "    def load_memory(self, _):\n",
    "        return self.memory.load_memory_variables({})['history']\n",
    "\n",
    "    def invoke(self, question):\n",
    "        answer = self.chain.invoke(question)\n",
    "        self.memory.save_context({\"input\": question}, {\"output\": answer.content})\n",
    "        \n",
    "        return answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 8단계 : `체인(Chain) 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_document_memory_chain = StuffDocumentMemoryChain(llm, prompt, memory, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📢 [ 질문 1 ] Aaronson 은 유죄인가요 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제가 알기로는 Aaronson이 유죄임이 밝혀졌습니다.'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# [ 질문 1 ] Aaronson 은 유죄인가요 ?\n",
    "stuff_document_memory_chain.invoke('Is Aaronson guilty?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📢 [ 질문 2 ] 그가 테이블에 어떤 메시지를 썼나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"제가 찾은 문서에는 테이블 위에 손가락으로 먼지에 쓴 메시지가 있습니다. 그 메시지는 '2+2=5'입니다.\""
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# [ 질문 2 ] 그가 테이블에 어떤 메시지를 썼나요?\n",
    "stuff_document_memory_chain.invoke('What message did he write on the table?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📢 [ 질문 3 ] Julia 는 누구인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제시된 문서에서 Julia는 윈스턴과 관련된 인물입니다. 그녀는 윈스턴과 함께 이야기의 중심에 있으며, 윈스턴이 그녀에 대한 강한 감정을 품고 있는 것으로 나타납니다. Julia는 윈스턴에게 중요한 인물로서, 그의 행동과 감정에 영향을 미치는 역할을 합니다.'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# [ 질문 3 ] Julia 는 누구인가요?\n",
    "stuff_document_memory_chain.invoke('Who is Julia?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
