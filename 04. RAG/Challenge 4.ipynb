{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📖 Stuff Documents 체인을 사용하여 완전한 RAG 파이프라인을 구현\n",
    "\n",
    "- 체인을 `수동으로 구현`해야 합니다.\n",
    "\n",
    "- 체인에 `ConversationBufferMemory` 를 부여합니다.\n",
    "\n",
    "- 해당 문서를 사용하여 RAG를 수행합니다 : https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\n",
    "\n",
    "- 체인에 다음 질문을 합니다:\n",
    "    - Aaronson 은 유죄인가요?\n",
    "    - 그가 테이블에 어떤 메시지를 썼나요?\n",
    "    - Julia 는 누구인가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 사용할 라이브러리 및 모듈 import 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder \n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 언어모델(LLM) 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,        # 창의성 (0 ~ 2)\n",
    "    model='gpt-3.5-turbo',  # 사용 모델 지정 (Default : gpt-3.5-turbo)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 메모리 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,           # 문자열 기반이 아닌, ChatPromptTemplate 에서 사용할 수 있는 형태로 반환\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 1단계 : `문서 로드 (Load Documents)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"./files/challenge.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 2단계 : `텍스트 분할 (Split Text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tkdgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tkdgu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",                 # 특정 기준으로 분할\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "split_doc = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 3단계 : `임베딩 (Embedding) 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 캐시 지정\n",
    "cache_dir = LocalFileStore('./.cache/')\n",
    "\n",
    "# 캐시를 활용한 임베딩\n",
    "cacehd_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 4단계 : `DB 생성 및 저장`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=split_doc, embedding=cacehd_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 5단계 : `검색기(Retriever) 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 6단계 : `프롬프트(prompt) 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\"\n",
    "    You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved {context} to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Answer in Korean.\n",
    "    \"\"\"\n",
    "    ),\n",
    "    # MessagesPlaceholder(variable_name='history'),\n",
    "    (\"human\", \"{question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 7단계 : `Stuff Documenet Chain 안에 Memory를 연결한 Class 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StuffDocumentMemoryChain:\n",
    "    def __init__(self, llm, prompt, memory, retriver, input_key=\"question\"):\n",
    "        self.prompt = prompt\n",
    "        self.memory = memory\n",
    "        self.retriver = retriver\n",
    "        self.input_key = input_key\n",
    "        self.chain = {\n",
    "            'context': retriver, \n",
    "            'history': self.load_memory,\n",
    "            'question': RunnablePassthrough(),\n",
    "        } | prompt | llm\n",
    "\n",
    "    def load_memory(self, _):\n",
    "        return self.memory.load_memory_variables({})['history']\n",
    "\n",
    "    def invoke(self, question):\n",
    "        answer = self.chain.invoke(question)\n",
    "        self.memory.save_context({\"input\": question}, {\"output\": answer.content})\n",
    "        \n",
    "        return answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 8단계 : `체인(Chain) 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_document_memory_chain = StuffDocumentMemoryChain(llm, prompt, memory, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📢 [ 질문 1 ] Aaronson 은 유죄인가요 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제가 알기로는, Aaronson은 유죄로 판결받았습니다.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# [ 질문 1 ] Aaronson 은 유죄인가요 ?\n",
    "stuff_document_memory_chain.invoke('Is Aaronson guilty?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📢 [ 질문 2 ] 그가 테이블에 어떤 메시지를 썼나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그는 테이블 위에 손가락으로 먼지에 흔적을 남겼고 \"2+2=5\"라고 쓴 것을 추적했습니다.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# [ 질문 2 ] 그가 테이블에 어떤 메시지를 썼나요?\n",
    "stuff_document_memory_chain.invoke('What message did he write on the table?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📢 [ 질문 3 ] Julia 는 누구인가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'제시된 문서에서 Julia는 윈스턴과 관련된 인물로서, 그의 사랑이자 동료입니다. 그녀는 윈스턴과 함께 Party에 저항하고자 하는 의지를 나타내며, 윈스턴이 자신을 버리고 그녀를 대신해서 고통을 받기를 바라는 마음을 품게 합니다. Julia는 윈스턴에게 중요한 인물로서, 그의 행동과 마음을 변화시키는데 영향을 미치는 역할을 합니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# [ 질문 3 ] Julia 는 누구인가요?\n",
    "stuff_document_memory_chain.invoke('Who is Julia?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📢 메모리를 출력하여 메모리가 체인에 적용되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Is Aaronson guilty?'),\n",
       " AIMessage(content='제가 알기로는, Aaronson은 유죄로 판결받았습니다.'),\n",
       " HumanMessage(content='What message did he write on the table?'),\n",
       " AIMessage(content='그는 테이블 위에 손가락으로 먼지에 흔적을 남겼고 \"2+2=5\"라고 쓴 것을 추적했습니다.'),\n",
       " HumanMessage(content='Who is Julia?'),\n",
       " AIMessage(content='제시된 문서에서 Julia는 윈스턴과 관련된 인물로서, 그의 사랑이자 동료입니다. 그녀는 윈스턴과 함께 Party에 저항하고자 하는 의지를 나타내며, 윈스턴이 자신을 버리고 그녀를 대신해서 고통을 받기를 바라는 마음을 품게 합니다. Julia는 윈스턴에게 중요한 인물로서, 그의 행동과 마음을 변화시키는데 영향을 미치는 역할을 합니다.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_document_memory_chain.load_memory({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
